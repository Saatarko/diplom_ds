# üß≠ RL Multi-Agent Maze Prototype  

**–ü—Ä–æ–µ–∫—Ç: –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞ –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º —Ñ—É–Ω–∫—Ü–∏–π: –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ä–µ–¥—ã, —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –≤ —Å—Ä–µ–¥–µ, –º–µ—Ç–∞-—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ.**  

---

## üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ–µ–∫—Ç–∞

### –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è + –Ω–∞–≤–∏–≥–∞—Ü–∏—è)

#### –õ–æ–∫–∞–ª—å–Ω–æ:

- –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
- –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏ —Å –¥–∏—Å–∫–∞

```
https://drive.google.com/drive/folders/15SN9VTYMs74h12f8yNwtEcAY2Q9ZwfYn?usp=sharing
```

- –ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω—Ñ–µ—Ä–µ–Ω—Å (replay_box/replay_full.py):

```
python replay_full.py
```

### –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∏–µ

#### –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è (–∫—Ä–æ–º–µ –º–µ—Ç–∞–∞–≥–µ–Ω—Ç–∞)

- –ö–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π
- –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞:
```
python maze_generator_cell_train.py 
```
- –∑–∞–ø—É—Å—Ç–∏—Ç—å  –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞–≤–∏–≥–∞—Ç–æ—Ä–∞ 
```
python maze_solver.py 
```

#### –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è –º–µ—Ç–∞–∞–≥–µ–Ω—Ç–∞:

- –†–∞—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –≤ meta_maze.py –∑–∞–º–µ–Ω–∏—Ç—å (–¥–ª—è –ø–µ—Ä–≤—ã—Ö 1000 -2000 –∏—Ç–µ—Ä–∞—Ü–∏–π) —Ä–µ–∞–ª—å–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –∏ –æ–±—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤ –∑–∞–≥–ª—É—à–∫–∞–º–∏ (–¥–ª—è —Å–µ—Ä—å–µ–∑–Ω–æ–π —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏)
- –∑–∞–ø—É—Å—Ç–∏—Ç—å meta_maze.py
------
## üõ† –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
- RL —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏: Gymnasium, Stable Baselines3 (PPO, MaskablePPO)
- ML-–±–∏–±–ª–∏–æ—Ç–µ–∫–∏: PyTorch
- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã: Pygame

–ü–æ–¥—Ö–æ–¥—ã: Curriculum learning, Adversarial Curriculum Learning

-------
## üéì 1. –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞

–ó–∞–¥–∞—á–∞ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ä–µ–¥—ã –≤ —Ä–∞–º–∫–∞—Ö –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning, RL) —è–≤–ª—è–µ—Ç—Å—è —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤. –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –ª–∞–±–∏—Ä–∏–Ω—Ç–æ–≤ –º–æ–∂–µ—Ç –∫–∞–∑–∞—Ç—å—Å—è —É–ø—Ä–æ—â—ë–Ω–Ω–æ–π –∏–ª–∏ –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–æ–π –∑–∞–¥–∞—á–µ–π, –¥–∞–Ω–Ω—ã–π –ø—Ä–æ–µ–∫—Ç —Ä–µ—à–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã—Ö –∏ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò –∏ RL:

1.1 –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–∞ –∏ —Å—Ä–µ–¥—ã
–ü—Ä–æ–µ–∫—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–∏–∫–ª–∞ ¬´–∞–≥–µ–Ω—Ç ‚Üî —Å—Ä–µ–¥–∞¬ª, –≥–¥–µ —Å—Ä–µ–¥–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π, –∞ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞—ë—Ç—Å—è –¥—Ä—É–≥–∏–º RL-–∞–≥–µ–Ω—Ç–æ–º. –≠—Ç–æ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –¥–≤—É—Å—Ç–æ—Ä–æ–Ω–Ω—é—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é, —á—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–º –∑–∞–¥–∞—á–∞–º, –≥–¥–µ –æ–∫—Ä—É–∂–∞—é—â–∞—è —Å—Ä–µ–¥–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –∏–∑–º–µ–Ω—á–∏–≤–æ–π –∏–ª–∏ –≤—Ä–∞–∂–¥–µ–±–Ω–æ–π (adversarial).

1.2 –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ä–µ–¥—ã –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω–∞—è RL-–∑–∞–¥–∞—á–∞
–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –ª–∞–±–∏—Ä–∏–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ—Ö–æ–¥–∏–º–æ—Å—Ç–∏, —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏–º–µ—Ä–æ–º –æ–±—É—á–µ–Ω–∏—è —Å—Ä–µ–¥—ã ‚Äî –∞–∫—Ç–∏–≤–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—â–µ–≥–æ—Å—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤ AI/ML. –≠—Ç–æ –ø—Ä–∏–±–ª–∏–∂–∞–µ—Ç –ø—Ä–æ–µ–∫—Ç –∫ –∑–∞–¥–∞—á–∞–º –≤—Ä–æ–¥–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∏–∑–∞–π–Ω–∞ —É—Ä–æ–≤–Ω–µ–π –≤ –∏–≥—Ä–∞—Ö (AI-assisted game design) –∏ –æ–±—É—á–∞—é—â–∏—Ö —Å–∏–º—É–ª—è—Ç–æ—Ä–æ–≤.

1.3 –°urriculum learning –∏ adversarial –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è.
–ü—Ä–æ–µ–∫—Ç –≤–Ω–µ–¥—Ä—è–µ—Ç adversarial curriculum learning: –ª–∞–±–∏—Ä–∏–Ω—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ —É—Å–ª–æ–∂–Ω—è—é—Ç—Å—è –≤ –æ—Ç–≤–µ—Ç –Ω–∞ —É—Å–ø–µ—Ö–∏ –Ω–∞–≤–∏–≥–∞—Ç–æ—Ä–∞, —á—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤—É—é —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è. –í–≤–µ–¥–µ–Ω–∏–µ –º–µ—Ç–∞-–∞–≥–µ–Ω—Ç–∞, —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ —ç—Ç–∏–º –ø—Ä–æ—Ü–µ—Å—Å–æ–º, —Ä–∞—Å—à–∏—Ä—è–µ—Ç –ø—Ä–æ–µ–∫—Ç –¥–æ –º–µ—Ç–∞-RL, –æ–¥–Ω–æ–≥–æ –∏–∑ —Å–∞–º—ã—Ö –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π (learning to learn).

1.4 –ù–∞–≤–∏–≥–∞—Ü–∏—è –∫–∞–∫ –±–∞–∑–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –≤ —Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–µ –∏ CV
–ü—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –ª–∞–±–∏—Ä–∏–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏, –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Ä–æ–±–æ—Ç–æ–≤ –∏–ª–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –º–∞—à–∏–Ω, –Ω–æ —Å –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ–º –ø–æ–ª–Ω–æ–π –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ—Å—Ç–∏ –∏ –º–∞–ª–æ–π –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –ø—Ä–æ–µ–∫—Ç:

- —Å–ª—É–∂–∏—Ç –ø—Ä–æ—Ç–æ—Ç–∏–ø–æ–º –¥–ª—è –∑–∞–¥–∞—á —Ä–µ–∞–ª—å–Ω–æ–≥–æ –º–∏—Ä–∞ (path planning, obstacle avoidance);
- –∏–∑–±–µ–≥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã —Å –≥—Ä–æ–º–æ–∑–¥–∫–∏–º–∏ CV-–¥–∞–Ω–Ω—ã–º–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ —Ä–µ–∞–ª–∏–∑—É–µ–º—ã–º –∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è (2 –º–µ—Å—è—Ü–∞);
- —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –≥–ª—É–±–∏–Ω—É, —Å—Ä–∞–≤–Ω–∏–º—É—é —Å –±–æ–ª–µ–µ –¥–æ—Ä–æ–≥–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏.

1.5 –ì–∏–±–∫–æ—Å—Ç—å, –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å –∏ –Ω–∞—É—á–Ω–∞—è —Ü–µ–Ω–Ω–æ—Å—Ç—å
–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ä–µ–¥—ã, —á—Ç–æ –¥–µ–ª–∞–µ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—É—é —á–∞—Å—Ç—å –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ–π, –ª–µ–≥–∫–æ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–π –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–π.

### üìå –í—ã–≤–æ–¥:
–ü—Ä–æ–µ–∫—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç:
- —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –≤ RL (multi-agent, curriculum, meta-learning);
- –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –ø–æ—Å—Ç–∞–Ω–æ–≤–∫—É –∑–∞–¥–∞—á–∏ (–∞–≥–µ–Ω—Ç —Å—Ç—Ä–æ–∏—Ç —Å—Ä–µ–¥—É –¥–ª—è –¥—Ä—É–≥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞);
- –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—É—é –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –±–∞–∑—É.

## ü§ñ 2. –°–≤—è–∑—å —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏:

- Path planning –≤ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∞–≤—Ç–æ / –¥—Ä–æ–Ω–∞—Ö ‚Äî –ø–æ—Ö–æ–∂–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å;
- –†–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞
- –°–µ—Ç–µ–≤–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è
- –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –≤ –∏–≥—Ä–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, RTS) ‚Äî —Ç–µ –∂–µ —ç–ª–µ–º–µ–Ω—Ç—ã: –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–∞—Ä—Ç—ã + –∞–≥–µ–Ω—Ç;
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–¥–∞–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ‚Äî meta-learning;
- Auto-curriculum –∏ self-play ‚Äî –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ AlphaZero, OpenAI Five, etc.

## üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å CV/—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–æ–π

| –ö—Ä–∏—Ç–µ—Ä–∏–π                      | –ù–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ –ª–∞–±–∏—Ä–∏–Ω—Ç—É                  | CV/—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞            |
| ----------------------------- | --------------------------------------- | --------------------------- |
| –°–≤–æ—è —Å—Ä–µ–¥–∞                    | ‚úÖ –õ–µ–≥–∫–æ —Å–¥–µ–ª–∞—Ç—å –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å          | ‚ùå –°–ª–æ–∂–Ω–æ, –¥–æ—Ä–æ–≥–æ            |
| –ö–æ–Ω—Ç—Ä–æ–ª—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏            | ‚úÖ –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å (—á–µ—Ä–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä)     | ‚ùå –ù–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –∫–µ–π—Å—ã     |
| –ö–æ–ª-–≤–æ –¥–∞–Ω–Ω—ã—Ö                 | ‚úÖ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–∞ –ª–µ—Ç—É                    | ‚ùå –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã     |
| –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è             | ‚úÖ –ü—Ä–∏–µ–º–ª–µ–º–∞—è                            | ‚ùå –û—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ            |
| –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å             | ‚úÖ –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è, –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–∞—è          | ‚ùå –ó–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–µ–Ω—Å–æ—Ä–æ–≤, —à—É–º–∞ |
| –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–Ω–µ–¥—Ä–µ–Ω–∏—è meta-RL | ‚úÖ –ü—Ä—è–º–æ –ø–æ —Å—Ö–µ–º–µ builder/navigator/meta | ‚ùå –°–ª–æ–∂–Ω–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å      |


## 3. –õ–æ–≥–∏–∫–∞ —Ç—Ä—ë—Ö –∞–≥–µ–Ω—Ç–æ–≤

### 3.1  –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä
- –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ä–µ–¥—ã –∏—Å–ø—ã—Ç–∞–Ω–∏–π (–ª–∞–±–∏—Ä–∏–Ω—Ç–æ–≤).
- –£—Å–ª–æ–∂–Ω—è–µ—Ç –∏–ª–∏ —É–ø—Ä–æ—â–∞–µ—Ç –æ–∫—Ä—É–∂–µ–Ω–∏–µ, —Ä–µ–∞–≥–∏—Ä—É—è –Ω–∞ —É—Å–ø–µ—Ö–∏/–Ω–µ—É–¥–∞—á–∏ –Ω–∞–≤–∏–≥–∞—Ç–æ—Ä–∞.
- –ü—Ä–∏ —Ä–∞–±–æ—Ç–µ –∏–º–µ—Ç –¥–≤–µ —Ñ–∞–∑—ã: –∫–æ–ø–∞–Ω–∏–µ –∏ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –ø–æ–ª—É—á–∞–µ—Ç –∏–∑ –∫–∞—Ä—Ç—ã –ª–∞–±–∏—Ä–∏–Ω—Ç–∞ –∏ 
—Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã –Ω–∞–≥—Ä–∞–¥

–í –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∞–Ω–∞–ª–æ–≥–∏—è—Ö: 

- —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–∏—Å—Ç–µ–º–∞, –∫–æ—Ç–æ—Ä–∞—è –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å—Ü–µ–Ω–∞—Ä–∏–∏/—Å–∏—Ç—É–∞—Ü–∏–∏ ‚Äî –¥–æ—Ä–æ–∂–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –¥–ª—è –±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫–æ–≤, 
- —Ç–æ–ø–æ–ª–æ–≥–∏–∏ –¥–ª—è —Å–µ—Ç–µ–≤–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏, 
- –∑–∞–¥–∞–Ω–∏—è –¥–ª—è —Ä–æ–±–æ—Ç–æ–≤.

–¢–æ –µ—Å—Ç—å –æ–Ω —Å–∏–º—É–ª–∏—Ä—É–µ—Ç –º–∏—Ä, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –ø—Ä–µ–æ–¥–æ–ª–µ—Ç—å.

### 3.2 –ù–∞–≤–∏–≥–∞—Ç–æ—Ä

–ó–∞–¥–∞—á–∞ ‚Äî –Ω–∞—Ö–æ–¥–∏—Ç—å –ø—É—Ç—å –≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ä–µ–¥–µ.
–û–Ω –æ–ø–µ—Ä–∏—Ä—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –ø–æ–∏—Å–∫–∞ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–∞—Ä—à—Ä—É—Ç–∞ (–ø–æ —Å—É—Ç–∏ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É pathfinding / planning).
–û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –ø–æ–ª—É—á–∞–µ—Ç –∏–∑ –∫–∞—Ä—Ç—ã –ª–∞–±–∏—Ä–∏–Ω—Ç–∞ –∏ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã –Ω–∞–≥—Ä–∞–¥ (—Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏—è –≤ —Ç—É–ø–∏–∫–∞—Ö)

–í –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∞–Ω–∞–ª–æ–≥–∏—è—Ö:

- –î–ª—è —Ä–æ–±–æ—Ç–∞ ‚Üí –Ω–∞–≤–∏–≥–∞—Ü–∏—è –≤ –ø–æ–º–µ—â–µ–Ω–∏–∏.
- –î–ª—è —Å–µ—Ç–∏ ‚Üí –≤—ã–±–æ—Ä –º–∞—Ä—à—Ä—É—Ç–∞ –¥–æ—Å—Ç–∞–≤–∫–∏ –ø–∞–∫–µ—Ç–æ–≤.
- –î–ª—è –ª–æ–≥–∏—Å—Ç–∏–∫–∏ ‚Üí –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ü–µ–ø–æ—á–∫–∏ –ø–æ—Å—Ç–∞–≤–æ–∫.

- –ì–ª–∞–≤–Ω–æ–µ ‚Äî –æ–Ω –Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç —Å—Ä–µ–¥—É, –æ–Ω –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è.

### 3.3 –ú–µ—Ç–∞-–∞–≥–µ–Ω—Ç

–≠—Ç–æ ¬´–¥–∏—Ä–∏–∂—ë—Ä¬ª –∏–ª–∏ ¬´–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä¬ª.

–û–Ω –Ω–µ –∑–Ω–∞–µ—Ç –¥–µ—Ç–∞–ª–µ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ–¥—á–∏–Ω—ë–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤, –Ω–æ —É–º–µ–µ—Ç:
- —Ä–µ—à–∞—Ç—å, –∫–æ–≥–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å,
- –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã,
- –º–µ–Ω—è—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤,
- –∏–Ω–∏—Ü–∏–∏—Ä–æ–≤–∞—Ç—å –¥–æ–æ–±—É—á–µ–Ω–∏–µ –Ω—É–∂–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞.

–ò–¥–µ—è –≤–∑—è—Ç–∞ –∏–∑ Adversarial Curriculum Learning (ACL): –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä ¬´–±—Ä–æ—Å–∞–µ—Ç –≤—ã–∑–æ–≤¬ª, –Ω–∞–≤–∏–≥–∞—Ç–æ—Ä –ø—ã—Ç–∞–µ—Ç—Å—è —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è, –∞ –º–µ—Ç–∞-–∞–≥–µ–Ω—Ç —Ä–µ—à–∞–µ—Ç –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –Ω–∏–º–∏.

–í –ø—Ä–∏–∫–ª–∞–¥–Ω—ã—Ö –∞–Ω–∞–ª–æ–≥–∏—è—Ö:

- –í –±–µ—Å–ø–∏–ª–æ—Ç–Ω–∏–∫–∞—Ö ‚Üí –º–µ—Ç–∞-–∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç —Ä–µ—à–∞—Ç—å: —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –ª–∏ —Å–µ–π—á–∞—Å perception-–º–æ–¥—É–ª—å (CV), –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª—å.
- –í –∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ ‚Üí –∞–≥–µ–Ω—Ç –º–æ–∂–µ—Ç —Ä–µ—à–∞—Ç—å: —Å—Ç–æ–∏—Ç –ª–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∞—Ç–∞–∫ (Red Team) –∏–ª–∏ –Ω–∞–≤–∏–≥–∞—Ç–æ—Ä-–∑–∞—â–∏—Ç–Ω–∏–∫ (Blue Team).
- –í —Å–µ—Ç–µ–≤—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö ‚Üí –º–µ—Ç–∞-–∞–≥–µ–Ω—Ç —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Å–∏–º—É–ª—è—Ç–æ—Ä —Å–µ—Ç–∏ –∏ ¬´–º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä¬ª.

### 3.3.1 –ê–Ω–∞–ª–æ–≥–∏—è —Å Mass Effect: –õ–µ–≥–∏–æ–Ω üõ°Ô∏è

–õ–µ–≥–∏–æ–Ω ‚Äî —ç—Ç–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –∞–≥–µ–Ω—Ç–æ–≤, –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö –æ–±—â–µ–π —Ü–µ–ª—å—é.
–£ –Ω–∞—Å  ‚Äî —Å–∏—Å—Ç–µ–º–∞ –∞–≥–µ–Ω—Ç–æ–≤, –≥–¥–µ –∫–∞–∂–¥—ã–π —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –Ω–æ –º–µ—Ç–∞-–∞–≥–µ–Ω—Ç –∫–æ–æ—Ä–¥–∏–Ω–∏—Ä—É–µ—Ç –∏—Ö –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞.
¬´–°–∏—Å—Ç–µ–º–∞ –Ω–∞–ø–æ–º–∏–Ω–∞–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø "–∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞", –≥–¥–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∞–≥–µ–Ω—Ç—ã –≤—ã–ø–æ–ª–Ω—è—é—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –∞ —É–ø—Ä–∞–≤–ª—è—é—â–∏–π —Å–ª–æ–π –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—é –∏—Ö –¥–µ–π—Å—Ç–≤–∏–π¬ª.

# üß≠ RL Multi-Agent Maze Prototype

**Project: Research of a prototype of a multi-agent architecture with separation of functions: environment generation, task solution in the environment, meta-management.**

---

## üöÄ Launch the project

### Demo inference (generation + navigation)

#### Locally:

- Clone repository
- Load models from disk

```
https://drive.google.com/drive/folders/15SN9VTYMs74h12f8yNwtEcAY2Q9ZwfYn?usp=sharing
```

- Run inference (replay_box/replay_full.py):

```
python replay_full.py
```

### Self-training

#### For training from scratch (except metaagent)

- Clone repository
- Run generator training:
```
python maze_generator_cell_train.py
```
- run for navigator training
```
python maze_solver.py
```

For training a meta-agent from scratch:

- Rationally in meta_maze.py replace (for the first 1000-2000 iterations) real inference and agent training with stubs (for significant time savings)
- run meta_maze.py
------
## üõ† Technologies used
- RL frameworks: Gymnasium, Stable Baselines3 (PPO, MaskablePPO)
- ML libraries: PyTorch
- Visualization and interfaces: Pygame

Approaches: Curriculum learning, Adversarial Curriculum Learning

-------
## üéì 1. Justification of the relevance of the project

The task of navigation and environment generation within the framework of reinforcement learning (RL) is fundamental for the development of adaptive intelligent agents. While solving a maze may seem like a simplistic or academic task, this project addresses several key and current research areas in AI and RL:

1.1 Study of agent-environment interaction
The project demonstrates a full-fledged simulation of the agent ‚Üî environment cycle, where the environment is not fixed but dynamically generated by another RL agent. This opens up the possibility of exploring two-way adaptation, which corresponds to real-world problems where the environment can be changeable or adversarial.

1.2 Environment generation as a separate RL task
Developing a maze generator that learns based on complexity and passability is an example of environment learning, a rapidly developing area in AI/ML. This brings the project closer to problems such as AI-assisted game design and educational simulators.

1.3 Curriculum learning and adversarial interactions.
The project implements adversarial curriculum learning: mazes become progressively more complex in response to the navigator's success, which allows for a more stable learning trajectory to be built. The introduction of a meta-agent that controls this process expands the project to meta-RL, one of the most promising areas (learning to learn).

1.4 Navigation as a Basic Task in Robotics and CV
Navigating mazes simulates tasks similar to the navigation of robots or autonomous machines, but with the advantage of complete controllability and low computational cost. Thus, the project:

- serves as a prototype for real-world tasks (path planning, obstacle avoidance);
- avoids the need to work with cumbersome CV data, which makes it feasible in a limited time (2 months);
- maintains a research depth comparable to more expensive tasks.

5. Flexibility, reproducibility and scientific value
We use our own environments, which makes the experimental part reproducible, easily scalable and fully controllable.

### üìå Conclusion:
The project demonstrates:
- modern approaches in RL (multi-agent, curriculum, meta-learning);
- original problem statement (an agent builds an environment for another agent);
- reproducible research base.

## ü§ñ 2. Connection with real-world tasks:

- Path planning in autonomous cars / drones - similar algorithmic level;
- Robotics
- Network routing
- Agent training in a game (e.g. RTS) - the same elements: map generator + agent;
- Generation of tasks for training - meta-learning;
- Auto-curriculum and self-play - used in AlphaZero, OpenAI Five, etc.

## üìä Comparison with CV/robotics

| Criterion | Maze navigation | CV/robotics |
| ----------------------------- | --------------------------------------- | -------------------------- |
| Own environment | ‚úÖ Easy to make and adapt | ‚ùå Complex, expensive |
| Complexity control | ‚úÖ Full control (via generator) | ‚ùå Unpredictable cases |
| Amount of data | ‚úÖ Generate on the fly | ‚ùå Limited datasets |
| Training speed | ‚úÖ Acceptable | ‚ùå Very slow |
| Reproducibility | ‚úÖ Laboratory, controlled | ‚ùå Dependent on sensors, noise |
| Possibility of implementing meta-RL | ‚úÖ Directly according to the builder/navigator/meta scheme | ‚ùå Difficult to integrate |

## 3. Three-agent logic

### 3.1 Generator
- Responsible for generating the testing environment (mazes).
- Complicates or simplifies the environment, reacting to the navigator's successes/failures.
- Has two phases during operation: digging and placing elements. Receives main observations from the labirinta and
reward heat map

In applied analogies:

- it can be a system that generates scenarios/situations ‚Äî road conditions for drones,
- topologies for network routing,
- tasks for robots.

That is, it simulates the world that needs to be overcome.

### 3.2 Navigator

The task is to find a path in the generated environment.
It operates with search and route planning algorithms (essentially solves the pathfinding / planning problem).
It receives its main observations from the maze map and the reward heat map (with protection against getting stuck in dead ends)

In applied analogies:

- For a robot ‚Üí indoor navigation.
- For a network ‚Üí choosing a route for delivering packages.
- For logistics ‚Üí supply chain optimization.

- The main thing is that it does not change the environment, it adapts.

### 3.3 Meta-agent

This is a "conductor" or "orchestrator".

It does not know the implementation details of subordinate agents, but it can:
- decide who to launch,
- evaluate results,
- change the complexity of scenarios,
- initiate additional training of the required agent.

The idea is taken from Adversarial Curriculum Learning (ACL): the generator "throws down a challenge", the navigator tries to cope, and the meta-agent decides the balance between them.

In applied analogies:

- In drones ‚Üí the meta-agent can decide whether to train the perception module (CV), planning or control.
- In cybersecurity ‚Üí the agent can decide whether to train the attack generator (Red Team) or the navigator-defender (Blue Team).
- In network systems ‚Üí the meta-agent regulates the load on the network simulator and the "router".

### 3.3.1 Analogy with Mass Effect: Legion üõ°Ô∏è

Legion is a set of agents united by a common goal.
We have a system of agents, where each is highly specialized, but the meta-agent coordinates them to achieve the result.
"The system resembles the principle of "collective intelligence", where individual agents perform specialized functions, and the control layer ensures the coordination of their actions."